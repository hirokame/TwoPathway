{"cells":[{"cell_type":"code","execution_count":null,"id":"ad91c0a2-7599-48fc-a4c9-e0c638c9c1a4","metadata":{"id":"ad91c0a2-7599-48fc-a4c9-e0c638c9c1a4"},"outputs":[],"source":["import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sklearn\n","from sklearn.svm import SVC\n","from sklearn.neural_network import MLPRegressor, MLPClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models, Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n","from scipy import stats as st\n","import pickle\n","import joblib\n","from datetime import datetime\n","from tqdm import tqdm\n","from sklearn.preprocessing import normalize\n"]},{"cell_type":"code","execution_count":null,"id":"f66DQavTi2Ge","metadata":{"id":"f66DQavTi2Ge"},"outputs":[],"source":["%load_ext cudf.pandas"]},{"cell_type":"code","execution_count":null,"id":"tBxyemYQ4eIN","metadata":{"id":"tBxyemYQ4eIN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"Muemd2wCJNlO","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5191,"status":"ok","timestamp":1716795572912,"user":{"displayName":"Graybiel","userId":"05160908792815017417"},"user_tz":-540},"id":"Muemd2wCJNlO","outputId":"535e7228-4643-4eb3-dff0-0302fefed177"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"id":"KtIy-8yMzyTe","metadata":{"id":"KtIy-8yMzyTe"},"outputs":[],"source":["!cp /content/drive/MyDrive/Dopamine-Astrocyte-Behavior/arial.ttf /usr/share/fonts/truetype/liberation"]},{"cell_type":"code","execution_count":null,"id":"-M-rLhyFwBeY","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2276,"status":"ok","timestamp":1716794619319,"user":{"displayName":"Graybiel","userId":"05160908792815017417"},"user_tz":-540},"id":"-M-rLhyFwBeY","outputId":"cf99531d-fe22-400b-8142-635cada03989"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', '/usr/share/fonts/truetype/liberation/arial.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', '/usr/share/fonts/truetype/humor-sans/Humor-Sans.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf']\n"]}],"source":["from matplotlib import font_manager\n","from matplotlib import rcParams\n","\n","os.system('fc-cache -f -v')\n","\n","# Add the font to matplotlib's font cache\n","font_files = font_manager.findSystemFonts(fontpaths='/usr/share/fonts/truetype/')\n","for font_file in font_files:\n","    font_manager.fontManager.addfont(font_file)\n","print(font_manager.findSystemFonts(fontpaths=None, fontext='ttf'))\n","rcParams['figure.figsize'] = [6, 4]\n","rcParams['font.size'] = 6\n","rcParams['pdf.fonttype'] = 42\n","rcParams['font.family'] = 'Arial'"]},{"cell_type":"code","execution_count":null,"id":"1QLC_tWVAFKT","metadata":{"id":"1QLC_tWVAFKT"},"outputs":[],"source":["\n","def heatmap(y_test,y_predicted,variable,model_type, labels):\n","    conf_matrix = confusion_matrix(y_test,y_predicted)\n","    classification_rep = classification_report(y_test, y_predicted)\n","    conf_matrix_normalized = normalize(conf_matrix, axis=1, norm='l1')\n","\n","    # Plot the confusion matrix\n","    # print(classification_rep)\n","    print(accuracy_score)\n","    os.makedirs(f'/content/drive/MyDrive/Dopamine-Astrocyte-Behavior/{today}-{task}', exist_ok=True)\n","\n","    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', vmin=0, vmax=0.8, xticklabels=labels, yticklabels=labels)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    splits = variable.split('-')\n","    title = f'Animal {model_type} Confusion Matrix: {splits[0]}-{splits[1]}'\n","    plt.title(title)\n","    plt.savefig(f'/content/drive/MyDrive/Dopamine-Astrocyte-Behavior/{today}-{task}/{title}.pdf')\n","    #plt.savefig(f'/content/drive/MyDrive/Dopamine-Astrocyte-Behavior/{today}-{task}/{title}.svg')\n","    conf_matrix_df = pd.DataFrame(conf_matrix_normalized, index=labels, columns=labels)\n","    conf_matrix_df.to_csv(f'/content/drive/MyDrive/Dopamine-Astrocyte-Behavior/{today}-{task}/conf_matrix_normalized_{title}.csv'.format())\n","    plt.show()\n","\n","variable_names = ['Green-L-z(Ast)', 'Green-R-z(Ast)', 'Red-L-z(DA)', 'Red-R-z(DA)']\n","def get_variable_data(variable, all_necessary_dfs):\n","    all_necessary_dfs_per_variable = []\n","    for df in all_necessary_dfs:\n","        all_necessary_dfs_per_variable.append(df[variable_names[variable]][:-1].T)\n","    return all_necessary_dfs_per_variable\n"]},{"cell_type":"code","execution_count":null,"id":"Cs0he4H9fUlp","metadata":{"id":"Cs0he4H9fUlp"},"outputs":[],"source":["\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.ensemble import RandomForestClassifier\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n","\n","from tensorflow.keras.layers import Dropout\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.utils.class_weight import compute_class_weight\n","from keras.callbacks import ReduceLROnPlateau\n","\n","from tqdm import tqdm\n","def run_me(task, under_sample, all_necessary_dfs, labels):\n","    accuracies = pd.DataFrame(index = variable_names, columns = ['Accuracy'])\n","    accuraciescnn = pd.DataFrame(index = variable_names, columns = ['Accuracy'])\n","    accuraciesRFC = pd.DataFrame(index = variable_names, columns = ['Accuracy'])\n","\n","    epochs = 500\n","    batch_size = 30\n","    for i in tqdm(range(4)):\n","      variable = i\n","      all_necessary_dfs_per_variable = get_variable_data(variable, all_necessary_dfs)\n","\n","      combined_data = pd.DataFrame()\n","      min_sample = np.min([df.shape[0] for df in all_necessary_dfs_per_variable])\n","      for idx, all_necessary_df_per_variable in enumerate(all_necessary_dfs_per_variable):\n","\n","          all_necessary_df_per_variable['port'] = idx\n","          if under_sample ==False:\n","              combined_data = pd.concat([combined_data,all_necessary_df_per_variable], axis =0)\n","          elif under_sample ==True:\n","          # it is undersample to fix the data imbalance problem.\n","              combined_data =pd.concat([combined_data, all_necessary_df_per_variable.sample(frac=min_sample/len(all_necessary_df_per_variable), random_state=42)], axis =0)\n","      X= combined_data.drop(columns=['port'])\n","      X = X.fillna(0)\n","      X = X.values\n","      X = np.array(X)\n","      # replace nan values into 0\n","      y = combined_data['port'].values\n","      X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3, random_state = 77)\n","      print(\"X shape:\", X.shape)\n","      print(\"y shape:\", y.shape)\n","      svm_classifier = SVC()\n","      # Define the parameter grid\n","      param_grid = {\n","      'C': [0.1, 1, 10],  # Example values for C\n","      'gamma': [0.001, 0.01, 0.1],  # Example values for gamma\n","      'kernel': ['linear', 'poly', 'sigmoid']\n","      }\n","\n","      model = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy', verbose=0)\n","\n","      # Fit the classifier to the training data\n","      model.fit(X_train, y_train)\n","\n","      class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","\n","      class_weights = dict(enumerate(class_weights))\n","      # Initialize the GridSearchCV object\n","      rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50)\n","\n","      modelcnn = Sequential()\n","      modelcnn.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n","      modelcnn.add(MaxPooling1D(pool_size=2))\n","      modelcnn.add(Dropout(0.3))  # Add dropout layer\n","      modelcnn.add(Flatten())\n","      modelcnn.add(Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))  # Add L2 regularization\n","      modelcnn.add(Dropout(0.3))  # Add dropout layer\n","      modelcnn.add(Dense(len(np.unique(y_train)), activation='softmax'))\n","\n","      # Compile the model\n","      modelcnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","      # Train the model\n","      modelcnn.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_test, y_test), verbose=0, class_weight=class_weights)\n","\n","      accuracy = model.score(X_test, y_test)\n","      _, accuracycnn = modelcnn.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n","      print(i)\n","\n","      rf_classifier = RandomForestClassifier()\n","      rf_classifier.fit(X_train, y_train)\n","      # Predict the event type on test data\n","      y_pred_rf = rf_classifier.predict(X_test)\n","      accuracyrfc = accuracy_score(y_test, y_pred_rf)\n","\n","      accuracies.iloc[i]['Accuracy'] = accuracy\n","      accuraciescnn.iloc[i]['Accuracy'] = accuracycnn\n","      accuraciesRFC.iloc[i]['Accuracy'] = accuracyrfc\n","\n","      y_predicted = model.predict(X_test)\n","      y_predictedcnn = np.argmax(modelcnn.predict(X_test),axis = -1)\n","\n","      #LSTM\n","\n","\n","\n","      heatmap(y_test,y_predicted, variable_names[i],model_type = f'{animal_for_this_code} {task} SVC', labels= labels)\n","      heatmap(y_test,y_predictedcnn, variable_names[i],model_type = f'{animal_for_this_code} {task} CNN', labels =labels)\n","      heatmap(y_test,y_pred_rf, variable_names[i],model_type = f'{animal_for_this_code} {task} RFC', labels= labels)\n","\n","\n","      # Encode labels to integers\n","      label_encoder = LabelEncoder()\n","      y_encoded = label_encoder.fit_transform(y)\n","\n","      # Split the data into training and testing sets\n","      X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n","\n","      # Reshape X to be 3D as required by LSTM (samples, timesteps, features)\n","      X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","      X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","      # Compute class weights\n","      class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","\n","      class_weights = dict(enumerate(class_weights))\n","      # Build the LSTM model\n","      model = Sequential()\n","      model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n","      model.add(Dropout(0.3))\n","      model.add(LSTM(32, return_sequences=False))\n","      model.add(Dropout(0.3))\n","      model.add(Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n","      model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n","      early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","      # Compile the model\n","      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","      # Early stopping callback\n","      early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n","\n","      # Train the model with early stopping\n","      history = model.fit(X_train, y_train, epochs=300, batch_size=16, validation_data=(X_test, y_test),\n","                          callbacks=[early_stopping],class_weight=class_weights, verbose=0)\n","\n","      # Evaluate the model\n","      y_pred = model.predict(X_test)\n","      y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","      # Print classification report\n","      heatmap(y_test,y_pred_classes, variable_names[i],model_type = f'{animal_for_this_code} {task} LSTM', labels =labels)\n","          # print(y_predictedcnn)"]},{"cell_type":"code","execution_count":null,"id":"Tvb_aYiIAbFt","metadata":{"id":"Tvb_aYiIAbFt"},"outputs":[],"source":["def run_all_for_each_animal(animal_for_this_code, task, all_necessary_dfs):\n","\n","    task_name = task\n","    run_me(task= task_name, under_sample = False, all_necessary_dfs = all_necessary_dfs, labels=labels)\n","\n","    task_name = task  +\" undersample\"\n","    run_me(task = task_name, under_sample = True, all_necessary_dfs = all_necessary_dfs, labels =labels)"]},{"cell_type":"code","execution_count":null,"id":"b6MjYEoSvXNO","metadata":{"id":"b6MjYEoSvXNO"},"outputs":[],"source":["lists = ['Entries(even if out of task)', 'In Turn Area_left', 'In Turn Area_right', 'left entry', 'right entry']\n","labels = [\"Entries\", \"L-turn\", \"R-turn\", 'L-choice', 'R-choice']"]},{"cell_type":"code","execution_count":null,"id":"5sh12Tvs-2X1","metadata":{"id":"5sh12Tvs-2X1"},"outputs":[],"source":["\n","import os\n","\n","data_path = \"/content/drive/MyDrive/Dopamine-Astrocyte-Behavior/Pkl_files_for_SVM/240527_event_by_event\"\n","\n","for filename in os.listdir(data_path):\n","    if \"_high\" in filename or \"_low\" in filename or \"_middle\" in filename:\n","        os.remove(os.path.join(data_path, filename))\n","\n"]},{"cell_type":"code","execution_count":null,"id":"P9-L5ePb_zFV","metadata":{"id":"P9-L5ePb_zFV"},"outputs":[],"source":["import os\n","\n","data_path = \"/content/drive/MyDrive/Dopamine-Astrocyte-Behavior/Pkl_files_for_SVM/240527_event_by_event\"\n","\n","for filename in os.listdir(data_path):\n","    if \"entry_exit\" in filename or \"Entries(even if out of task)_exit\" in filename or \"In Turn Area_exit\" in filename:\n","        os.remove(os.path.join(data_path, filename))"]},{"cell_type":"code","execution_count":null,"id":"kBFwzUmG_nJt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716795847615,"user":{"displayName":"Graybiel","userId":"05160908792815017417"},"user_tz":-540},"id":"kBFwzUmG_nJt","outputId":"82e4c6c7-9a93-41a4-ed2e-65c8bd611488"},"outputs":[{"data":{"text/plain":["['240527_NTS3_right entry.pkl',\n"," '240527_NTS5_right entry.pkl',\n"," '240527_NTS6_right entry.pkl',\n"," '240527_PNOC1_right entry.pkl',\n"," '240527_NTS3_Entries(even if out of task).pkl',\n"," '240527_PNOC5_left entry.pkl',\n"," '240527_PNOC1_left entry.pkl',\n"," '240527_NTS6_Entries(even if out of task).pkl',\n"," '240527_PNOC5_Entries(even if out of task).pkl',\n"," '240527_PNOC2_left entry.pkl',\n"," '240527_PNOC3_Entries(even if out of task).pkl',\n"," '240527_PNOC3_left entry.pkl',\n"," '240527_PNOC2_right entry.pkl',\n"," '240527_PNOC5_right entry.pkl',\n"," '240527_NTS6_left entry.pkl',\n"," '240527_NTS2_left entry.pkl',\n"," '240527_NTS3_left entry.pkl',\n"," '240527_NTS2_right entry.pkl',\n"," '240527_NTS5_Entries(even if out of task).pkl',\n"," '240527_PNOC3_right entry.pkl',\n"," '240527_NTS5_left entry.pkl',\n"," '240527_PNOC4_right entry.pkl',\n"," '240527_PNOC4_Entries(even if out of task).pkl',\n"," '240527_PNOC2_Entries(even if out of task).pkl',\n"," '240527_PNOC1_Entries(even if out of task).pkl',\n"," '240527_NTS2_Entries(even if out of task).pkl',\n"," '240527_PNOC4_left entry.pkl',\n"," '240527_PNOC1_Left correct+rewarded.pkl',\n"," '240527_PNOC2_Left correct+rewarded.pkl',\n"," '240527_PNOC3_Left correct+rewarded.pkl',\n"," '240527_PNOC4_Left correct+rewarded.pkl',\n"," '240527_PNOC5_Left correct+rewarded.pkl',\n"," '240527_PNOC1_Right correct+rewarded.pkl',\n"," '240527_PNOC2_Right correct+rewarded.pkl',\n"," '240527_PNOC3_Right correct+rewarded.pkl',\n"," '240527_PNOC4_Right correct+rewarded.pkl',\n"," '240527_PNOC5_Right correct+rewarded.pkl',\n"," '240527_PNOC1_Left correct+omission.pkl',\n"," '240527_PNOC2_Left correct+omission.pkl',\n"," '240527_PNOC3_Left correct+omission.pkl',\n"," '240527_PNOC4_Left correct+omission.pkl',\n"," '240527_PNOC5_Left correct+omission.pkl',\n"," '240527_PNOC1_Right correct+omission.pkl',\n"," '240527_PNOC2_Right correct+omission.pkl',\n"," '240527_PNOC3_Right correct+omission.pkl',\n"," '240527_PNOC4_Right correct+omission.pkl',\n"," '240527_PNOC5_Right correct+omission.pkl',\n"," '240527_PNOC1_Left error.pkl',\n"," '240527_PNOC2_Left error.pkl',\n"," '240527_PNOC3_Left error.pkl',\n"," '240527_PNOC4_Left error.pkl',\n"," '240527_PNOC5_Left error.pkl',\n"," '240527_PNOC1_Right error.pkl',\n"," '240527_PNOC2_Right error.pkl',\n"," '240527_PNOC3_Right error.pkl',\n"," '240527_PNOC4_Right error.pkl',\n"," '240527_PNOC5_Right error.pkl',\n"," '240527_PNOC1_Airpuff.pkl',\n"," '240527_PNOC2_Airpuff.pkl',\n"," '240527_PNOC3_Airpuff.pkl',\n"," '240527_PNOC4_Airpuff.pkl',\n"," '240527_PNOC5_Airpuff.pkl',\n"," '240527_NTS2_Left correct+rewarded.pkl',\n"," '240527_NTS3_Left correct+rewarded.pkl',\n"," '240527_NTS5_Left correct+rewarded.pkl',\n"," '240527_NTS6_Left correct+rewarded.pkl',\n"," '240527_NTS2_Right correct+rewarded.pkl',\n"," '240527_NTS3_Right correct+rewarded.pkl',\n"," '240527_NTS5_Right correct+rewarded.pkl',\n"," '240527_NTS6_Right correct+rewarded.pkl',\n"," '240527_NTS2_Left correct+omission.pkl',\n"," '240527_NTS3_Left correct+omission.pkl',\n"," '240527_NTS5_Left correct+omission.pkl',\n"," '240527_NTS6_Left correct+omission.pkl',\n"," '240527_NTS2_Right correct+omission.pkl',\n"," '240527_NTS3_Right correct+omission.pkl',\n"," '240527_NTS5_Right correct+omission.pkl',\n"," '240527_NTS6_Right correct+omission.pkl',\n"," '240527_NTS2_Left error.pkl',\n"," '240527_NTS3_Left error.pkl',\n"," '240527_NTS5_Left error.pkl',\n"," '240527_NTS6_Left error.pkl',\n"," '240527_NTS2_Right error.pkl',\n"," '240527_NTS3_Right error.pkl',\n"," '240527_NTS5_Right error.pkl',\n"," '240527_NTS6_Right error.pkl',\n"," '240527_NTS2_Airpuff.pkl',\n"," '240527_NTS3_Airpuff.pkl',\n"," '240527_NTS5_Airpuff.pkl',\n"," '240527_NTS6_Airpuff.pkl',\n"," '240527_PNOC1_In Turn Area_left_exit.pkl',\n"," '240527_PNOC1_In Turn Area_right_exit.pkl',\n"," '240527_PNOC2_In Turn Area_right_exit.pkl',\n"," '240527_PNOC2_In Turn Area_left_exit.pkl',\n"," '240527_PNOC3_In Turn Area_right_exit.pkl',\n"," '240527_PNOC3_In Turn Area_left_exit.pkl',\n"," '240527_PNOC4_In Turn Area_right_exit.pkl',\n"," '240527_PNOC4_In Turn Area_left_exit.pkl',\n"," '240527_PNOC5_In Turn Area_left_exit.pkl',\n"," '240527_PNOC5_In Turn Area_right_exit.pkl',\n"," '240527_NTS2_In Turn Area_left_exit.pkl',\n"," '240527_NTS2_In Turn Area_right_exit.pkl',\n"," '240527_NTS3_In Turn Area_left_exit.pkl',\n"," '240527_NTS3_In Turn Area_right_exit.pkl',\n"," '240527_NTS5_In Turn Area_left_exit.pkl',\n"," '240527_NTS5_In Turn Area_right_exit.pkl',\n"," '240527_NTS6_In Turn Area_right_exit.pkl',\n"," '240527_NTS6_In Turn Area_left_exit.pkl']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(data_path)"]},{"cell_type":"code","execution_count":null,"id":"KcZe8ota_MJz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":661,"status":"ok","timestamp":1716795533525,"user":{"displayName":"Graybiel","userId":"05160908792815017417"},"user_tz":-540},"id":"KcZe8ota_MJz","outputId":"1e60d9b1-b4fa-4024-c197-3d94fbdb28da"},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["glob.glob(os.path.join(data_path, \"*_high*\"))"]},{"cell_type":"code","execution_count":null,"id":"iColGpXy7vyl","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"iColGpXy7vyl","outputId":"836736b9-9068-4799-ae7a-5f5d3e860b2f"},"outputs":[],"source":["task = \"task classification 4s all animals\"\n","today = datetime.today().strftime('%y%m%d')\n","print(today, task)\n","animal_types = [\"NTS\", \"PNOC\"]\n","\n","for animal_type in animal_types:\n","    if animal_type == \"PNOC\":\n","        animal_numbers = [1,2,3,4,5]\n","    elif animal_type == \"NTS\":\n","        animal_numbers = [2,3,5,6]\n","    for animal_number in tqdm(animal_numbers):\n","        animal_for_this_code = animal_type + str(animal_number) + \"_\"\n","        print(animal_for_this_code)\n","\n","        file_paths = sorted(os.listdir(data_path))\n","        files_ = []\n","        for file in sorted(file_paths):\n","            if animal_for_this_code in file:\n","                files_.append(file)\n","        all_necessary_dfs = []\n","        for list_ in lists:\n","            concat_all = pd.DataFrame()\n","            #labels.append(list_[:-5])\n","            len_files = 0\n","            for file_ in tqdm(files_):\n","                if list_ in file_:\n","                    concat_all = pd.concat([concat_all, pd.read_pickle(os.path.join(data_path, file_))], axis =1)\n","                    len_files+=1\n","                    print(file_)\n","            print(len_files)\n","            all_necessary_dfs.append(concat_all)\n","\n","        run_all_for_each_animal(animal_for_this_code,task=task, all_necessary_dfs=all_necessary_dfs)"]},{"cell_type":"code","execution_count":null,"id":"T6LSgQ5KZ8Iu","metadata":{"id":"T6LSgQ5KZ8Iu"},"outputs":[],"source":["animal_types = [ \"PNOC\", \"NTS\"]\n","task = \"task classification 4s all animals\"\n","today = datetime.today().strftime('%y%m%d')\n","print(today, task)\n","print(today, task)\n","for animal_type in animal_types:\n","    if animal_type == \"PNOC\":\n","        animal_numbers = [1,2,3,4,5 ]\n","    elif animal_type == \"NTS\":\n","        animal_numbers = [2,3,5,6]\n","    for animal_number in tqdm(animal_numbers):\n","        animal_for_this_code = animal_type + \"_\" +str(animal_number) + \"_\"\n","        print(animal_for_this_code)\n","\n","        file_paths = sorted(os.listdir(data_path))\n","        files_ = []\n","        for file in sorted(file_paths):\n","            if animal_for_this_code in file:\n","                files_.append(file)\n","\n","\n","        all_necessary_dfs = []\n","        for list_ in lists:\n","            concat_all = pd.DataFrame()\n","            #labels.append(list_[:-5])\n","            len_files = 0\n","            for file_ in tqdm(files_):\n","                if list_ in file_ and \"(1)\" not in file_ and \" 2 \" not in file_:\n","                    concat_all = pd.concat([concat_all, pd.read_pickle(os.path.join(data_path, file_))], axis =1)\n","                    len_files+=1\n","            print(len_files)\n","            all_necessary_dfs.append(concat_all)\n","\n","        for idx, df in enumerate(all_necessary_dfs):\n","            if idx <=1:\n","              all_necessary_dfs[idx] = df.loc[:, df.iloc[-1] < 0]\n","            if idx >=2:\n","              all_necessary_dfs[idx] = df.loc[:, df.iloc[-1] > 0]\n","\n","        run_all_for_each_animal(animal_for_this_code,task=task, all_necessary_dfs=all_necessary_dfs)"]},{"cell_type":"code","execution_count":null,"id":"NmmHBRmDKAl2","metadata":{"id":"NmmHBRmDKAl2"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1RKQH_Nm424D4bQz2klmPsPx5kN6yvEz8","timestamp":1716794557893},{"file_id":"1R5S__RaQqKs3eDHoRxMqxKPMt8pGTMAO","timestamp":1716791158258},{"file_id":"1CswVsOY84IPe-ehShQRCYQf4JCxYb1Vm","timestamp":1716451859678},{"file_id":"1xWzuYu0niOtlR0F-aUlJfS9o7gwlreHP","timestamp":1716445356100},{"file_id":"1QF7TC8aLMSZ2ax24lj6NXOGRrXJy-2O_","timestamp":1716441357684},{"file_id":"1iVmGEbirpA7ktLC-g0Fjn2rd7tmacgMo","timestamp":1716440723802},{"file_id":"1WDHkrn1a6ncCX2oina8F-pwnSak1V3wP","timestamp":1716356264678},{"file_id":"1sZtRdO1a2A9nFCFmDq9xPWFchyW-zcMN","timestamp":1716263206226}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}